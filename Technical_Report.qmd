---
title: "DATA 624 Project 2: Technical Report"
format: html
author: "Ali Ahmed, Andreina Arias, Kaylie Evans, Naomi Buell, and Zaneta Paulusova"
date: "`r Sys.Date()`"
---

```{r}
#| label: load packages
library(DataExplorer)
library(tidyverse)
library(readxl)
library(fpp3)
library(caret)
library(skimr)
library(doParallel)
library(RWeka)
library(rpart)
```

## Instructions

*This is role playing. I am your new boss. I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me. My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.*

*Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.*

*Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports. Also submit the excel file showing the prediction of your models for pH.*

## Introduction

In this technical report, we will analyze the manufacturing process of ABC Beverage to understand the factors that influence the pH level of the product. We will build predictive models to forecast pH levels based on various predictors in the dataset. The goal is to provide insights into the manufacturing process and to create a model that can be used for future predictions.

## Load data

First, we load the data.

```{r}
#| label: load-data

StudentData <- read_excel("StudentData.xlsx") # Training data
StudentEvaluation <- read_excel("StudentEvaluation.xlsx") # Test data
```

We browse the data below.

```{r}
#| label: browse-data

StudentData |> skim()
StudentEvaluation |> skim()
```

## Clean and Tidy Data

Mutate variable types (e.g., convert character column `Brand Code` to factor, numeric, etc., so we can use this to forecast data).

Browse and remove outliers if necessary.

Impute and transform if necessary.

-   Andreina to impute

-   Adreina to check correlation, remove features that have correlation over some threshold

-   Remove near zero variance predictors with `nearZeroVar()`.

## Exploratory Data Analysis

<!--# Ali, since this data is not a time series, we can't do time plots or decomp like we discussed earlier. You can try exploring and plotting the data using `featurePlot(trainingData$x, trainingData$y)`, a correlation plot between all variables with `ggpairs()`, and/or other methods? If relationships between predictors and PH look linear, we'd opt for linear models. If relationships look non-linear, we can try forecasting with non-linear or tree-based models. -Naomi -->

-   Ali to Plot time plot

-   Ali to plot decomposition plot
```{r}
summary(student_data)
plot_histogram(student_data)

student_data |>
  summarise(across(everything(), ~ sum(is.na(.)))) |>
  pivot_longer(everything()) |>
  filter(value > 0) |>
  arrange(desc(value))

plot_missing(student_data)


## Forecast

We will try the following models:

-   Linear regression models:

    -   PLS

    -   Elastic Net

-   Non-linear regression models:

    -   KNN

    -   Multivariate Adaptive Regression Splines (MARS)

-   Tree-based regression models:

    -   Single Tree

    -   Model Tree

Since the data is relatively large, we will avoid heavy models like neural networks, support vector machines (SVM), or boosted trees. Since we have a lot of predictors, we opt for models that handle feature selection or dimensionality reduction like elastic net, PLS, and tree-based models. We also want model results to be easy to interpret for our non-technical report, so we focus on elastic net, MARS, and single/model trees.

<!--# Based on correlation plot in EDA section above, if relationships between predictors and PH look linear, we may opt for linear models. If relationships look non-linear, we can try forecasting with non-linear or tree-based models. -Naomi -->

Also, because the data is relatively large, we can afford to use a training and a test set.

```{r}
#| label: split-data

# <!--NB to DELETE this line after imputation section is completed-->
StudentData <- StudentData |> drop_na()

# Split the data into a training and a test set
trainingRows <- createDataPartition(
    StudentData$PH,
    p = .80,
    list = FALSE
)
StudentData_train <- StudentData[trainingRows, ]
StudentData_test <- StudentData[-trainingRows, ]

# Save training and test predictors and response variables
StudentData_train_x <- StudentData_train[, !names(StudentData_train) %in% "PH"]
StudentData_train_y <- StudentData_train$PH
StudentData_test_x <- StudentData_test[, !names(StudentData_test) %in% "PH"]
StudentData_test_y <- StudentData_test$PH
```

Since these models take a long time to train, we'll utilize the `doParallel` package for speed.

```{r}
#| label: parallel-processing
#| warning: FALSE
#| message: FALSE

cluster <- makeCluster(
    detectCores() - 1
)

registerDoParallel(
    cluster
)
```

Fitting models to the training data below.

```{r}
#| label: single-tree
#| warning: false

# Train
rpartTune <- train(
    StudentData_train_x,
    StudentData_train_y,
    method = "rpart2",
    tuneLength = 10,
    trControl = trainControl(method = "cv")
)
rpartTune

# Predict
rpartPred <- predict(
    rpartTune,
    newdata = StudentData_test_x
)
```

```{r}
#| label: model-tree
#| warning: false

# <!--NB to DELETE this line after data cleaning section is completed and variable types are changed to numeric-->
StudentData_train_x <- StudentData_train_x[, sapply(
    StudentData_train_x,
    is.numeric
)]

# Train
m5Tune <- train(
    StudentData_train_x,
    StudentData_train_y,
    method = "M5",
    trControl = trainControl(method = "cv"),
    control = Weka_control(M = 10)
)
m5Tune

# Predict
m5Pred <- predict(
    m5Tune,
    newdata = StudentData_test_x
)
```

## Compare model fit and select optimal model

Next, we compare fit and pick model to export.

```{r}
#| label: nonlinear-regression-models

ranking <- data.frame(
    Model = c(
        "Single Tree",
        "Model Tree"
    ),
    rbind(
        postResample(pred = rpartPred, obs = StudentData_test_y),
        postResample(pred = m5Pred, obs = StudentData_test_y)
    )
) |>
    arrange(RMSE)
ranking
best <- ranking[1, 1]
```

**The `r best` model gives the optimal resampling and test set performance, as it has the lowest RMSE, highest R squared, and lowest MAE.**

## Export

```{r}
#| label: Export to Excel

```

## Conclusion

\[RP?\]
